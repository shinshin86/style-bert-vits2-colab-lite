import os
import shutil
import subprocess
import sys
import yaml
import gradio as gr

# Style-Bert-VITS2ã®ãƒªãƒã‚¸ãƒˆãƒªãŒCloneã•ã‚Œã‚‹å ´æ‰€
REPO_ROOT = "/content/Style-Bert-VITS2"

# REPO_ROOTãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
if not os.path.exists(REPO_ROOT):
    raise RuntimeError(f"Style-Bert-VITS2ã®ãƒªãƒã‚¸ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {REPO_ROOT}")

# æœ€åˆã«REPO_ROOTã«ç§»å‹•
os.chdir(REPO_ROOT)
sys.path.insert(0, REPO_ROOT)

# å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
dataset_root = "/content/dataset/Style-Bert-VITS2/Data"
assets_root = "/content/dataset/Style-Bert-VITS2/model_assets"
input_dir = "/content/dataset/Style-Bert-VITS2/inputs"

os.makedirs(dataset_root, exist_ok=True)
os.makedirs(assets_root, exist_ok=True)
os.makedirs(input_dir, exist_ok=True)
os.makedirs("configs", exist_ok=True)

# paths.ymlã‚’ä½œæˆ
with open("configs/paths.yml", "w", encoding="utf-8") as f:
    yaml.dump({"dataset_root": dataset_root, "assets_root": assets_root}, f)

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from gradio_tabs.train import preprocess_all, get_path
from style_bert_vits2.nlp.japanese import pyopenjtalk_worker


def run_pipeline(
    model_name: str,
    initial_prompt: str,
    use_jp_extra: bool,
    batch_size: int,
    epochs: int,
    save_every_steps: int,
    normalize: bool,
    trim: bool,
    yomi_error: str,
    files: list[gr.File],
):
    """Gradioã‹ã‚‰å‘¼ã°ã‚Œã‚‹ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logs = []
    
    try:
        # 1. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’input_dirã«ã‚³ãƒ”ãƒ¼
        logs.append(f"ğŸ“ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ä¸­...")
        for f in files:
            dst = os.path.join(input_dir, os.path.basename(f.name))
            shutil.copy(f.name, dst)
        logs.append(f"âœ… {len(files)}å€‹ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ")
        
        # 2. slice.pyã‚’å®Ÿè¡Œ
        logs.append(f"\nğŸ”„ éŸ³å£°ã‚’åˆ†å‰²ä¸­...")
        result = subprocess.run(
            ["python", "slice.py", "-i", input_dir, "--model_name", model_name],
            capture_output=True,
            text=True
        )
        if result.returncode != 0:
            logs.append(f"âŒ ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            return None, "\n".join(logs)
        logs.append("âœ… éŸ³å£°åˆ†å‰²å®Œäº†")
        
        # 3. transcribe.pyã‚’å®Ÿè¡Œ
        logs.append(f"\nğŸ”„ éŸ³å£°ã‚’æ›¸ãèµ·ã“ã—ä¸­...")
        result = subprocess.run(
            [
                "python", "transcribe.py", 
                "--model_name", model_name,
                "--initial_prompt", initial_prompt,
                "--use_hf_whisper"
            ],
            capture_output=True,
            text=True
        )
        if result.returncode != 0:
            logs.append(f"âŒ ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            return None, "\n".join(logs)
        logs.append("âœ… æ›¸ãèµ·ã“ã—å®Œäº†")
        
        # 4. å‰å‡¦ç†ã‚’å®Ÿè¡Œ
        logs.append(f"\nğŸ”„ å‰å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
        pyopenjtalk_worker.initialize_worker()
        
        preprocess_all(
            model_name=model_name,
            batch_size=batch_size,
            epochs=epochs,
            save_every_steps=save_every_steps,
            num_processes=2,
            normalize=normalize,
            trim=trim,
            freeze_EN_bert=False,
            freeze_JP_bert=False,
            freeze_ZH_bert=False,
            freeze_style=False,
            freeze_decoder=False,
            use_jp_extra=use_jp_extra,
            val_per_lang=0,
            log_interval=200,
            yomi_error=yomi_error,
        )
        logs.append("âœ… å‰å‡¦ç†å®Œäº†")
        
        # 5. config.ymlã‚’ä½œæˆ
        logs.append(f"\nğŸ”„ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­...")
        paths = get_path(model_name)
        dataset_path = str(paths.dataset_path)
        config_path = str(paths.config_path)
        
        with open("default_config.yml", "r", encoding="utf-8") as f:
            yml_data = yaml.safe_load(f)
        yml_data["model_name"] = model_name
        with open("config.yml", "w", encoding="utf-8") as f:
            yaml.dump(yml_data, f, allow_unicode=True)
        logs.append("âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†")
        
        # 6. å­¦ç¿’ã‚’å®Ÿè¡Œ
        logs.append(f"\nğŸš€ å­¦ç¿’ã‚’é–‹å§‹ä¸­...")
        if use_jp_extra:
            cmd = [
                "python", "train_ms_jp_extra.py",
                "--config", config_path,
                "--model", dataset_path,
                "--assets_root", assets_root
            ]
        else:
            cmd = [
                "python", "train_ms.py",
                "--config", config_path,
                "--model", dataset_path,
                "--assets_root", assets_root
            ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            logs.append(f"âŒ ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            return None, "\n".join(logs)
        logs.append("âœ… å­¦ç¿’å®Œäº†")
        
        # 7. ONNXå¤‰æ›
        logs.append(f"\nğŸ”„ ONNXå¤‰æ›ä¸­...")
        # onnxå¤‰æ›ç”¨ã®ãƒ–ãƒ©ãƒ³ãƒã‚’ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆ
        if not os.path.exists("convert_onnx.py"):
            subprocess.run(["git", "fetch", "-p"], check=True)
            subprocess.run(["git", "checkout", "dev"], check=True)
            subprocess.run(["pip", "install", "-r", "requirements-colab.txt", "--timeout", "120"], check=True)
        
        result = subprocess.run(
            ["python", "convert_onnx.py", "--model", os.path.join(assets_root, model_name)],
            capture_output=True,
            text=True
        )
        if result.returncode != 0:
            logs.append(f"âŒ ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            return None, "\n".join(logs)
        logs.append("âœ… ONNXå¤‰æ›å®Œäº†")
        
        # 8. ZIPåŒ–
        logs.append(f"\nğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã‚’ZIPåŒ–ä¸­...")
        zip_path = os.path.join(assets_root, f"{model_name}.zip")
        shutil.make_archive(
            zip_path[:-4], 
            "zip", 
            os.path.join(assets_root, model_name)
        )
        logs.append(f"âœ… ZIPåŒ–å®Œäº†: {model_name}.zip")
        
        logs.append(f"\nğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        return zip_path, "\n".join(logs)
        
    except Exception as e:
        logs.append(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None, "\n".join(logs)


# Gradio UI
with gr.Blocks(title="Style-Bert-VITS2 Trainer") as demo:
    gr.Markdown("""
    # Style-Bert-VITS2 å­¦ç¿’ Web UI
    
    1. å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„
    2. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆWAVå½¢å¼ï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„
    3. ã€Œå­¦ç¿’ã‚’é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨å‡¦ç†ãŒå§‹ã¾ã‚Šã¾ã™
    4. å®Œäº†å¾Œã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™
    """)
    
    with gr.Row():
        with gr.Column():
            model_name_in = gr.Text(
                label="ãƒ¢ãƒ‡ãƒ«å",
                value="your_model_name",
                info="è‹±æ•°å­—ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿ä½¿ç”¨å¯èƒ½"
            )
            initial_prompt_in = gr.Text(
                label="åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
                value="ã“ã‚“ã«ã¡ã¯ã€‚å…ƒæ°—ã€ã§ã™ã‹ãƒ¼ï¼Ÿãµãµã£ã€ç§ã¯â€¦â€¦ã¡ã‚ƒã‚“ã¨å…ƒæ°—ã ã‚ˆï¼",
                info="æ›¸ãèµ·ã“ã—ã®ä¾‹æ–‡ï¼ˆå¥èª­ç‚¹ã‚„å›ºæœ‰åè©ã®æ›¸ãæ–¹ã®å‚è€ƒï¼‰"
            )
        
        with gr.Column():
            use_jp_extra_in = gr.Checkbox(
                label="JP-Extraã‚’æœ‰åŠ¹åŒ–",
                value=True,
                info="æ—¥æœ¬èªç‰¹åŒ–ç‰ˆï¼ˆè‹±èªãƒ»ä¸­å›½èªã¯ä½¿ç”¨ä¸å¯ï¼‰"
            )
            batch_size_in = gr.Number(
                label="ãƒãƒƒãƒã‚µã‚¤ã‚º",
                value=4,
                precision=0,
                info="VRAMã«å¿œã˜ã¦èª¿æ•´"
            )
            epochs_in = gr.Number(
                label="ã‚¨ãƒãƒƒã‚¯æ•°",
                value=100,
                precision=0,
                info="å­¦ç¿’ã®ç¹°ã‚Šè¿”ã—å›æ•°"
            )
    
    with gr.Row():
        with gr.Column():
            normalize_in = gr.Checkbox(label="éŸ³é‡æ­£è¦åŒ–", value=False)
            trim_in = gr.Checkbox(label="ç„¡éŸ³ãƒˆãƒªãƒ ", value=False)
        
        with gr.Column():
            save_every_in = gr.Number(
                label="ä¿å­˜é »åº¦ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ï¼‰",
                value=1000,
                precision=0
            )
            yomi_error_in = gr.Dropdown(
                label="èª­ã¿ã‚¨ãƒ©ãƒ¼æ™‚ã®æŒ™å‹•",
                choices=["raise", "skip", "use"],
                value="skip",
                info="raise:ã‚¨ãƒ©ãƒ¼ã§åœæ­¢, skip:è©²å½“è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—, use:ç„¡ç†ã‚„ã‚Šä½¿ç”¨"
            )
    
    audio_files_in = gr.Files(
        label="éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆWAVå½¢å¼ï¼‰",
        file_types=["audio"],
        file_count="multiple"
    )
    
    run_btn = gr.Button("ğŸš€ å­¦ç¿’ã‚’é–‹å§‹", variant="primary", size="lg")
    
    with gr.Row():
        result_zip_out = gr.File(label="å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆZIPï¼‰")
        logs_out = gr.Textbox(
            label="å‡¦ç†ãƒ­ã‚°",
            lines=20,
            max_lines=30,
            interactive=False
        )
    
    run_btn.click(
        fn=run_pipeline,
        inputs=[
            model_name_in,
            initial_prompt_in,
            use_jp_extra_in,
            batch_size_in,
            epochs_in,
            save_every_in,
            normalize_in,
            trim_in,
            yomi_error_in,
            audio_files_in,
        ],
        outputs=[result_zip_out, logs_out],
    )
    
    gr.Markdown("""
    ---
    ### ä½¿ã„æ–¹ã®ãƒ’ãƒ³ãƒˆ
    - éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¯10ç§’ä»¥ä¸Šã€åˆè¨ˆ10åˆ†ä»¥ä¸Šæ¨å¥¨
    - ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯å°ã•ãã—ã¦ãã ã•ã„
    - ã‚¨ãƒãƒƒã‚¯æ•°ã¯100ã§ååˆ†ãªå ´åˆãŒå¤šã„ã§ã™
    """)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--share", action="store_true", help="Gradio shareãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–")
    args = parser.parse_args()
    
    demo.launch(share=args.share, allowed_paths=[assets_root])